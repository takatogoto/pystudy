{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Tensors and autograd\n",
    "-------------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation computes the forward pass using operations on PyTorch\n",
    "Tensors, and uses PyTorch autograd to compute gradients.\n",
    "\n",
    "\n",
    "A PyTorch Tensor represents a node in a computational graph. If ``x`` is a\n",
    "Tensor that has ``x.requires_grad=True`` then ``x.grad`` is another Tensor\n",
    "holding the gradient of ``x`` with respect to some scalar value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32929920.0\n",
      "1 29555502.0\n",
      "2 28037760.0\n",
      "3 24574410.0\n",
      "4 18701992.0\n",
      "5 12247577.0\n",
      "6 7297312.0\n",
      "7 4245776.0\n",
      "8 2586848.0\n",
      "9 1710297.75\n",
      "10 1231201.25\n",
      "11 948238.75\n",
      "12 764829.0\n",
      "13 635288.625\n",
      "14 537635.8125\n",
      "15 460786.0\n",
      "16 398556.59375\n",
      "17 347227.1875\n",
      "18 304243.0\n",
      "19 267892.75\n",
      "20 236952.90625\n",
      "21 210457.578125\n",
      "22 187643.6875\n",
      "23 167881.171875\n",
      "24 150660.015625\n",
      "25 135593.65625\n",
      "26 122363.71875\n",
      "27 110703.6015625\n",
      "28 100395.3828125\n",
      "29 91259.2734375\n",
      "30 83126.421875\n",
      "31 75858.4453125\n",
      "32 69347.3046875\n",
      "33 63499.76171875\n",
      "34 58234.328125\n",
      "35 53481.3125\n",
      "36 49183.30078125\n",
      "37 45289.39453125\n",
      "38 41752.86328125\n",
      "39 38536.50390625\n",
      "40 35607.09375\n",
      "41 32934.68359375\n",
      "42 30492.271484375\n",
      "43 28256.955078125\n",
      "44 26209.03515625\n",
      "45 24330.794921875\n",
      "46 22605.44140625\n",
      "47 21018.984375\n",
      "48 19560.634765625\n",
      "49 18216.75\n",
      "50 16977.72265625\n",
      "51 15833.4853515625\n",
      "52 14776.3134765625\n",
      "53 13796.2587890625\n",
      "54 12888.72265625\n",
      "55 12048.033203125\n",
      "56 11268.39453125\n",
      "57 10545.177734375\n",
      "58 9873.97265625\n",
      "59 9250.3466796875\n",
      "60 8670.408203125\n",
      "61 8130.6083984375\n",
      "62 7628.169921875\n",
      "63 7160.037109375\n",
      "64 6723.54150390625\n",
      "65 6316.23828125\n",
      "66 5936.1875\n",
      "67 5581.2841796875\n",
      "68 5249.55908203125\n",
      "69 4939.453125\n",
      "70 4649.3779296875\n",
      "71 4377.83544921875\n",
      "72 4123.5830078125\n",
      "73 3885.482177734375\n",
      "74 3662.365966796875\n",
      "75 3453.2236328125\n",
      "76 3256.988037109375\n",
      "77 3072.83056640625\n",
      "78 2900.0234375\n",
      "79 2737.779541015625\n",
      "80 2585.37158203125\n",
      "81 2442.132080078125\n",
      "82 2307.42724609375\n",
      "83 2180.781005859375\n",
      "84 2061.669189453125\n",
      "85 1949.6376953125\n",
      "86 1844.1748046875\n",
      "87 1744.87158203125\n",
      "88 1651.3240966796875\n",
      "89 1563.146240234375\n",
      "90 1480.052001953125\n",
      "91 1401.7100830078125\n",
      "92 1327.8878173828125\n",
      "93 1258.2724609375\n",
      "94 1192.5718994140625\n",
      "95 1130.5618896484375\n",
      "96 1072.0\n",
      "97 1016.6864013671875\n",
      "98 964.41455078125\n",
      "99 915.026123046875\n",
      "100 868.3377075195312\n",
      "101 824.2164916992188\n",
      "102 782.492431640625\n",
      "103 743.0057983398438\n",
      "104 705.6528930664062\n",
      "105 670.2942504882812\n",
      "106 636.8409423828125\n",
      "107 605.1555786132812\n",
      "108 575.1510620117188\n",
      "109 546.7402954101562\n",
      "110 519.8196411132812\n",
      "111 494.3154602050781\n",
      "112 470.1250915527344\n",
      "113 447.1917724609375\n",
      "114 425.4504699707031\n",
      "115 404.8329162597656\n",
      "116 385.2731018066406\n",
      "117 366.7115173339844\n",
      "118 349.1075744628906\n",
      "119 332.388427734375\n",
      "120 316.52374267578125\n",
      "121 301.45904541015625\n",
      "122 287.149658203125\n",
      "123 273.55877685546875\n",
      "124 260.6474304199219\n",
      "125 248.3856658935547\n",
      "126 236.73008728027344\n",
      "127 225.6549072265625\n",
      "128 215.12149047851562\n",
      "129 205.10763549804688\n",
      "130 195.5841064453125\n",
      "131 186.5283203125\n",
      "132 177.9132843017578\n",
      "133 169.71873474121094\n",
      "134 161.92190551757812\n",
      "135 154.499755859375\n",
      "136 147.4342041015625\n",
      "137 140.7099151611328\n",
      "138 134.30853271484375\n",
      "139 128.2142791748047\n",
      "140 122.40909576416016\n",
      "141 116.8803939819336\n",
      "142 111.61329650878906\n",
      "143 106.5962142944336\n",
      "144 101.81444549560547\n",
      "145 97.25726318359375\n",
      "146 92.91426849365234\n",
      "147 88.77462768554688\n",
      "148 84.8286361694336\n",
      "149 81.0656967163086\n",
      "150 77.47754669189453\n",
      "151 74.053955078125\n",
      "152 70.791748046875\n",
      "153 67.6789779663086\n",
      "154 64.70895385742188\n",
      "155 61.87432098388672\n",
      "156 59.1701774597168\n",
      "157 56.58891677856445\n",
      "158 54.12693405151367\n",
      "159 51.775413513183594\n",
      "160 49.530487060546875\n",
      "161 47.3866081237793\n",
      "162 45.34001159667969\n",
      "163 43.38520050048828\n",
      "164 41.51860046386719\n",
      "165 39.735801696777344\n",
      "166 38.03354263305664\n",
      "167 36.40662384033203\n",
      "168 34.852210998535156\n",
      "169 33.36669158935547\n",
      "170 31.947729110717773\n",
      "171 30.589969635009766\n",
      "172 29.293312072753906\n",
      "173 28.05402374267578\n",
      "174 26.86928939819336\n",
      "175 25.736326217651367\n",
      "176 24.652708053588867\n",
      "177 23.61651039123535\n",
      "178 22.62557601928711\n",
      "179 21.678312301635742\n",
      "180 20.772241592407227\n",
      "181 19.905017852783203\n",
      "182 19.075681686401367\n",
      "183 18.28218650817871\n",
      "184 17.5231876373291\n",
      "185 16.796451568603516\n",
      "186 16.100704193115234\n",
      "187 15.435617446899414\n",
      "188 14.798015594482422\n",
      "189 14.18833065032959\n",
      "190 13.605215072631836\n",
      "191 13.046812057495117\n",
      "192 12.511204719543457\n",
      "193 11.998793601989746\n",
      "194 11.508062362670898\n",
      "195 11.038427352905273\n",
      "196 10.588505744934082\n",
      "197 10.157421112060547\n",
      "198 9.744855880737305\n",
      "199 9.349271774291992\n",
      "200 8.970256805419922\n",
      "201 8.607105255126953\n",
      "202 8.259722709655762\n",
      "203 7.926456451416016\n",
      "204 7.607128143310547\n",
      "205 7.301127910614014\n",
      "206 7.007744312286377\n",
      "207 6.726356029510498\n",
      "208 6.45680570602417\n",
      "209 6.198758125305176\n",
      "210 5.95100736618042\n",
      "211 5.713354110717773\n",
      "212 5.485794544219971\n",
      "213 5.26773738861084\n",
      "214 5.058185577392578\n",
      "215 4.857522964477539\n",
      "216 4.6649861335754395\n",
      "217 4.48048210144043\n",
      "218 4.303154945373535\n",
      "219 4.133566856384277\n",
      "220 3.9703927040100098\n",
      "221 3.8141705989837646\n",
      "222 3.6641242504119873\n",
      "223 3.520038604736328\n",
      "224 3.3819022178649902\n",
      "225 3.2494919300079346\n",
      "226 3.1224026679992676\n",
      "227 3.00032377243042\n",
      "228 2.88315486907959\n",
      "229 2.770698308944702\n",
      "230 2.6627261638641357\n",
      "231 2.55914044380188\n",
      "232 2.4598779678344727\n",
      "233 2.3644890785217285\n",
      "234 2.2727720737457275\n",
      "235 2.1848325729370117\n",
      "236 2.1002163887023926\n",
      "237 2.019152879714966\n",
      "238 1.9413862228393555\n",
      "239 1.8665038347244263\n",
      "240 1.7946645021438599\n",
      "241 1.7257410287857056\n",
      "242 1.6593244075775146\n",
      "243 1.5957447290420532\n",
      "244 1.5345346927642822\n",
      "245 1.4757713079452515\n",
      "246 1.4194034337997437\n",
      "247 1.3651460409164429\n",
      "248 1.3131144046783447\n",
      "249 1.2630155086517334\n",
      "250 1.2149720191955566\n",
      "251 1.1686466932296753\n",
      "252 1.1242856979370117\n",
      "253 1.0816929340362549\n",
      "254 1.0407698154449463\n",
      "255 1.0012789964675903\n",
      "256 0.9633286595344543\n",
      "257 0.9269781708717346\n",
      "258 0.8919990658760071\n",
      "259 0.858388364315033\n",
      "260 0.8260626196861267\n",
      "261 0.7949557304382324\n",
      "262 0.7650415301322937\n",
      "263 0.7363301515579224\n",
      "264 0.7087398171424866\n",
      "265 0.6821487545967102\n",
      "266 0.6565629243850708\n",
      "267 0.6320568323135376\n",
      "268 0.6083815693855286\n",
      "269 0.5856382250785828\n",
      "270 0.5638387799263\n",
      "271 0.5427952408790588\n",
      "272 0.5226044654846191\n",
      "273 0.5032027363777161\n",
      "274 0.48446229100227356\n",
      "275 0.46649909019470215\n",
      "276 0.44918790459632874\n",
      "277 0.4325379431247711\n",
      "278 0.4165795147418976\n",
      "279 0.4010947346687317\n",
      "280 0.3863263428211212\n",
      "281 0.37207725644111633\n",
      "282 0.358316034078598\n",
      "283 0.3451230227947235\n",
      "284 0.33242517709732056\n",
      "285 0.32019639015197754\n",
      "286 0.30835795402526855\n",
      "287 0.2970839738845825\n",
      "288 0.2861838936805725\n",
      "289 0.2757166624069214\n",
      "290 0.26559916138648987\n",
      "291 0.25584477186203003\n",
      "292 0.2465004324913025\n",
      "293 0.2374856173992157\n",
      "294 0.2288193255662918\n",
      "295 0.2204921394586563\n",
      "296 0.2124362736940384\n",
      "297 0.20470163226127625\n",
      "298 0.19728338718414307\n",
      "299 0.1900928020477295\n",
      "300 0.1831822395324707\n",
      "301 0.17652224004268646\n",
      "302 0.1701788455247879\n",
      "303 0.16398511826992035\n",
      "304 0.15803900361061096\n",
      "305 0.15229621529579163\n",
      "306 0.14680851995944977\n",
      "307 0.1415294110774994\n",
      "308 0.13640910387039185\n",
      "309 0.13151678442955017\n",
      "310 0.12677200138568878\n",
      "311 0.12220489978790283\n",
      "312 0.11780096590518951\n",
      "313 0.11355140805244446\n",
      "314 0.10948273539543152\n",
      "315 0.10556547343730927\n",
      "316 0.1017666831612587\n",
      "317 0.09814538061618805\n",
      "318 0.09462343901395798\n",
      "319 0.0912337377667427\n",
      "320 0.08796857297420502\n",
      "321 0.08482450991868973\n",
      "322 0.08180514723062515\n",
      "323 0.07889840751886368\n",
      "324 0.07608024030923843\n",
      "325 0.07337576150894165\n",
      "326 0.07074321806430817\n",
      "327 0.0682431012392044\n",
      "328 0.06581054627895355\n",
      "329 0.06349431723356247\n",
      "330 0.06123046949505806\n",
      "331 0.05906831845641136\n",
      "332 0.056955017149448395\n",
      "333 0.05494260787963867\n",
      "334 0.05302837863564491\n",
      "335 0.05114618316292763\n",
      "336 0.0493498370051384\n",
      "337 0.04760376736521721\n",
      "338 0.04592883959412575\n",
      "339 0.044310636818408966\n",
      "340 0.042755257338285446\n",
      "341 0.04124818742275238\n",
      "342 0.039803385734558105\n",
      "343 0.03841211646795273\n",
      "344 0.037062808871269226\n",
      "345 0.03577004373073578\n",
      "346 0.03453703597187996\n",
      "347 0.033333636820316315\n",
      "348 0.03215628117322922\n",
      "349 0.03102870285511017\n",
      "350 0.029961902648210526\n",
      "351 0.028912393376231194\n",
      "352 0.02789386361837387\n",
      "353 0.026928218081593513\n",
      "354 0.02600104920566082\n",
      "355 0.025105228647589684\n",
      "356 0.0242290198802948\n",
      "357 0.023393461480736732\n",
      "358 0.02258085459470749\n",
      "359 0.021814005449414253\n",
      "360 0.0210504699498415\n",
      "361 0.020326320081949234\n",
      "362 0.01963789388537407\n",
      "363 0.018963372334837914\n",
      "364 0.01832009106874466\n",
      "365 0.01769224926829338\n",
      "366 0.017085978761315346\n",
      "367 0.016505807638168335\n",
      "368 0.015937598422169685\n",
      "369 0.0153960594907403\n",
      "370 0.014874555170536041\n",
      "371 0.01436289306730032\n",
      "372 0.013875424861907959\n",
      "373 0.013408956117928028\n",
      "374 0.012961317785084248\n",
      "375 0.01251891627907753\n",
      "376 0.012096662074327469\n",
      "377 0.011684716679155827\n",
      "378 0.011295611970126629\n",
      "379 0.01092598121613264\n",
      "380 0.010561959818005562\n",
      "381 0.010203476995229721\n",
      "382 0.009862392209470272\n",
      "383 0.009537636302411556\n",
      "384 0.009217681363224983\n",
      "385 0.008914017118513584\n",
      "386 0.00861500296741724\n",
      "387 0.008332762867212296\n",
      "388 0.008053633384406567\n",
      "389 0.007794543169438839\n",
      "390 0.007538758218288422\n",
      "391 0.007290424779057503\n",
      "392 0.0070506734773516655\n",
      "393 0.0068231853656470776\n",
      "394 0.006600096821784973\n",
      "395 0.006382153369486332\n",
      "396 0.006178407464176416\n",
      "397 0.005981282331049442\n",
      "398 0.005787823814898729\n",
      "399 0.0056030056439340115\n",
      "400 0.005425885785371065\n",
      "401 0.005251313094049692\n",
      "402 0.005083642899990082\n",
      "403 0.004923263564705849\n",
      "404 0.00476892339065671\n",
      "405 0.004617126192897558\n",
      "406 0.004471446853131056\n",
      "407 0.004328020382672548\n",
      "408 0.004192104563117027\n",
      "409 0.0040639787912368774\n",
      "410 0.003937356173992157\n",
      "411 0.003818368073552847\n",
      "412 0.003698054701089859\n",
      "413 0.0035866412799805403\n",
      "414 0.0034744390286505222\n",
      "415 0.0033716722391545773\n",
      "416 0.0032709348015487194\n",
      "417 0.00316764903254807\n",
      "418 0.0030731973238289356\n",
      "419 0.0029811812564730644\n",
      "420 0.0028908720705658197\n",
      "421 0.0028084951918572187\n",
      "422 0.0027237837202847004\n",
      "423 0.002644874854013324\n",
      "424 0.0025658877566456795\n",
      "425 0.0024894974194467068\n",
      "426 0.0024186093360185623\n",
      "427 0.002348964801058173\n",
      "428 0.002281569642946124\n",
      "429 0.0022170438896864653\n",
      "430 0.0021539663430303335\n",
      "431 0.002093796618282795\n",
      "432 0.002033561235293746\n",
      "433 0.0019774879328906536\n",
      "434 0.001922087511047721\n",
      "435 0.0018700282089412212\n",
      "436 0.0018173460848629475\n",
      "437 0.001766999950632453\n",
      "438 0.0017185304313898087\n",
      "439 0.0016711719799786806\n",
      "440 0.0016257625538855791\n",
      "441 0.0015825608279556036\n",
      "442 0.0015402050921693444\n",
      "443 0.0015012971125543118\n",
      "444 0.0014589227503165603\n",
      "445 0.0014212605310603976\n",
      "446 0.001384926144964993\n",
      "447 0.0013490873388946056\n",
      "448 0.0013142179232090712\n",
      "449 0.0012797447852790356\n",
      "450 0.001247194828465581\n",
      "451 0.0012156262528151274\n",
      "452 0.001184709370136261\n",
      "453 0.0011554115917533636\n",
      "454 0.001125643844716251\n",
      "455 0.0010982777457684278\n",
      "456 0.0010729533387348056\n",
      "457 0.0010446810629218817\n",
      "458 0.001018966780975461\n",
      "459 0.00099375715944916\n",
      "460 0.0009697736240923405\n",
      "461 0.0009459065040573478\n",
      "462 0.0009226385154761374\n",
      "463 0.0009026615298353136\n",
      "464 0.0008816178306005895\n",
      "465 0.0008609530632384121\n",
      "466 0.0008393916068598628\n",
      "467 0.0008198324358090758\n",
      "468 0.0007995557971298695\n",
      "469 0.0007810774841345847\n",
      "470 0.0007641479605808854\n",
      "471 0.0007458839099854231\n",
      "472 0.000728875573258847\n",
      "473 0.0007121540838852525\n",
      "474 0.0006955278804525733\n",
      "475 0.0006795211811549962\n",
      "476 0.0006658499478362501\n",
      "477 0.0006503267795778811\n",
      "478 0.0006360800471156836\n",
      "479 0.0006228365818969905\n",
      "480 0.0006085428176447749\n",
      "481 0.0005953810177743435\n",
      "482 0.000583088374696672\n",
      "483 0.0005702298949472606\n",
      "484 0.0005582010489888489\n",
      "485 0.0005464840214699507\n",
      "486 0.0005354320746846497\n",
      "487 0.0005238017765805125\n",
      "488 0.0005128007614985108\n",
      "489 0.0005029975436627865\n",
      "490 0.0004920429782941937\n",
      "491 0.0004818771267309785\n",
      "492 0.00047291727969422936\n",
      "493 0.00046320405090227723\n",
      "494 0.00045396125642582774\n",
      "495 0.0004436234594322741\n",
      "496 0.0004349995870143175\n",
      "497 0.0004263615992385894\n",
      "498 0.0004177225928287953\n",
      "499 0.00041034570313058794\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
